{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Consensus Scoring using Protein Conformational Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "sys.path.append('..')\n",
    "from modules.run_or_load_decorator import run_or_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Load the data frame containing the raw docking scoring results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466, 402)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = './df_DkSc_results_COCRYS_CSAR_DEKOIS_DUD.pkl'\n",
    "X_merged_dksc = pd.read_pickle(file_name)\n",
    "# Extract activity column\n",
    "y_true_merged = X_merged_dksc['activity']\n",
    "# Drop column from merged_dkksc\n",
    "X_merged_dksc = X_merged_dksc.drop('activity', axis=1)\n",
    "X_merged_dksc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train test splitting murcko data frame\n",
    "Load the murcko scaffolds dataframe to perform scaffold splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('../2_Docking_analysis/')\n",
    "from scaffold_splitter import train_test_scaffold_split\n",
    "\n",
    "# Compute or load the dataframe containing the Generic Murcko Scaffolds\n",
    "file = '../2_Docking_analysis/df_COCRYS_CSAR_DUD_DEKOIS_Murcko_Scaffolds_SMILES.obj'\n",
    "\n",
    "df_scff_murcko = pd.read_pickle(file)\n",
    "df_scff_murcko.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read RFE Selectors to get the preselected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./5_Helper_get_RFE_preselected_conformations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LR_rand', 'RF_rand', 'XGB_rand', 'LR_scff', 'RF_scff', 'XGB_scff'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of been a list as with ML results, here selectors is a dictionary containing selector name and preselected conformations\n",
    "rfe_preselections.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import plotmetrics module to evaluate docking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../modules/plotting_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Consensus Scoring Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./5_Helper_Consensus_Scoring.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of parameters to evaluate Consensus scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none performs Consensus scoring using all molecules, scff and rand applies CS method over test set\n",
    "splitting_methods = ['none', 'scff', 'rand'] \n",
    "\n",
    "scaffold_series = df_scff_murcko['scff_generic']\n",
    "\n",
    "# Conf. Selectors\n",
    "selectors = ['rand', 'LR', 'RF', 'XGB']\n",
    "\n",
    "# Consensus Scoring Methods\n",
    "consensus_methods = {\n",
    "    'MEAN': get_mean_score,\n",
    "    'MED':  get_median_score,\n",
    "    'RANK': get_rank_score,\n",
    "    'MIN':  get_min_score,\n",
    "    'MAX':  get_max_score,\n",
    "    'EUN':  get_euc_norm_score,\n",
    "    'VOTE': get_vote_score,\n",
    "    'ECR':  get_exp_consensus_ranking\n",
    "}\n",
    "\n",
    "# List of parameters to compute\n",
    "roc_params = {'metric_name': 'roc_auc'}\n",
    "nef_params = {'metric_name': 'nef_auc'}\n",
    "pr_params = {'metric_name': 'pr_auc'}\n",
    "\n",
    "# The Ra value for the testing set in FXa is 75/1559 = 0.05\n",
    "# Therefore the maximum value of alpha for bedroc could be a=20\n",
    "bedroc_20 = {'metric_name': 'bedroc', 'alpha': 20}\n",
    "bedroc_10 = {'metric_name': 'bedroc', 'alpha': 10}\n",
    "bedroc_2 = {'metric_name': 'bedroc', 'alpha': 2}\n",
    "bedroc_05 = {'metric_name': 'bedroc', 'alpha': 0.5}\n",
    "\n",
    "# ef values 0.001, 0.005, 0.02, 0.1, 0.2\n",
    "ef_0001 = {'metric_name': 'ef', 'fraction': 0.001}\n",
    "ef_0005 = {'metric_name': 'ef', 'fraction': 0.005}\n",
    "ef_002 = {'metric_name': 'ef', 'fraction': 0.02}\n",
    "ef_02 = {'metric_name': 'ef', 'fraction': 0.2}\n",
    "\n",
    "# List of metrics\n",
    "metrics = [roc_params, nef_params, pr_params,\n",
    "           bedroc_20, bedroc_10, bedroc_2, bedroc_05,\n",
    "           ef_0001, ef_0005, ef_002, ef_02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-roc_auc.obj\n",
      "nef_auc\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-nef_auc.obj\n",
      "pr_auc\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-pr_auc.obj\n",
      "bedroc_20\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-bedroc_20.obj\n",
      "bedroc_10\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-bedroc_10.obj\n",
      "bedroc_2\n",
      "File loaded: ./consensus_scoring_results/CDK2_CS_results_conformational_selection-bedroc_2.obj\n",
      "bedroc_0.5\n",
      "none/rand/MEAN/bedroc_0.5\n",
      "none/rand/MED/bedroc_0.5\n",
      "none/rand/RANK/bedroc_0.5\n",
      "none/rand/MIN/bedroc_0.5\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prot_name = 'CDK2'\n",
    "base_filename = f'./consensus_scoring_results/{prot_name}_CS_results_conformational_selection'\n",
    "\n",
    "for metric_eval in metrics:\n",
    "    metric_name = '_'.join([str(i) for i in metric_eval.values()])\n",
    "    print(metric_name)\n",
    "    df = aggregate_conf_selection_results_CS(f'{base_filename}-{metric_name}.obj', \n",
    "                                             X, y, \n",
    "                                             splitting_methods=splitting_methods, \n",
    "                                             selectors=selectors,\n",
    "                                             cs_methods=consensus_methods, \n",
    "                                             metrics=[metric_eval], \n",
    "                                             nreps=10, \n",
    "                                             scaffold_series=scaffold_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "prot_name = 'CDK2'\n",
    "base_filename = f'./consensus_scoring_results/{prot_name}_CS_results_conformational_selection'\n",
    "\n",
    "files = glob(base_filename + '*')\n",
    "df = pd.concat([pd.read_pickle(i) for i in files]).round(4)\n",
    "df.to_pickle(f'./{prot_name}_dash_app_Consensus_results.obj')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
