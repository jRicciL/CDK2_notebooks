{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_results(path, column_to_drop, active_label='Active'):\n",
    "    '''Function to load VS results from path file.\n",
    "    It returns X and y ndarrays.'''\n",
    "    path_file = os.path.join(*path.split('/'))\n",
    "    df_results = pd.read_csv(path_file, index_col=0)\n",
    "    y_ = pd.Series(df_results[column_to_drop] == active_label, dtype = int) # Setting y_true\n",
    "    X_ = df_results.drop([column_to_drop], axis = 1) # Setting X\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docking Scores\n",
    "data_dir = '../data/docking_scores/'\n",
    "# vinardo Docking Scores (dksc) CSAR\n",
    "X_dksc_CSAR, y_dksc_CSAR = \\\n",
    "open_results(path = F'{data_dir}/vs_docking_crys_ensemble_VINARDO.csv', \n",
    "             column_to_drop = 'ActiveInactive')\n",
    "# Vinardo Docking Scores (dksc) CSAR\n",
    "X_dksc_DUD, y_dksc_DUD = \\\n",
    "open_results(path = F'{data_dir}/vs_docking_DUD2006_vs_402_crys_vinardo_8x.csv',\n",
    "             column_to_drop = 'Actividad')\n",
    "# Vianrdo Docking Scores (dksc) DEKOIS2\n",
    "X_dksc_DEKOIS, y_dksc_DEKOIS = \\\n",
    "open_results(path = F'{data_dir}/vs_dk_CRYS_402_DEKOIS_VINARDO_docking_score.csv',\n",
    "             column_to_drop = 'Actividad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ligand Efficiency scores\n",
    "data_dir = '../data/docking_scores/'\n",
    "# vinardo Docking Scores (dklef) CSAR\n",
    "X_dkef_CSAR, y_dkef_CSAR = \\\n",
    "open_results(path = F'{data_dir}/vs_dk_CRYS_402_CSAR_VINARDO_ligand_Efficiency.csv',\n",
    "            column_to_drop = 'ActiveInactive')\n",
    "# Vinardo Docking Scores (dklef) CSAR\n",
    "X_dkef_DUD, y_dkef_DUD = \\\n",
    "open_results(path = F'{data_dir}/vs_dk_CRYS_402_DUD2006_VINARDO_ligand_Efficiency.csv',\n",
    "            column_to_drop = 'Actividad')\n",
    "# Vianrdo Docking Scores (dklef) DEKOIS2\n",
    "X_dkef_DEKOIS, y_dkef_DEKOIS = \\\n",
    "open_results(path = F'{data_dir}/vs_dk_CRYS_402_DEKOIS2_VINARDO_ligand_Efficiency.csv',\n",
    "            column_to_drop = 'Actividad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of docking results (Only DkSc are included)\n",
    "scores_dic = {'CSAR': {'Dk_sc':   {'X': X_dksc_CSAR, 'y': y_dksc_CSAR}, \n",
    "                       'Dk_lef':  {'X': X_dkef_CSAR, 'y': y_dkef_CSAR} },\n",
    "              'DUD':  {'Dk_sc':   {'X': X_dksc_DUD,  'y': y_dksc_DUD}, \n",
    "                       'Dk_lef':  {'X': X_dkef_DUD,  'y': y_dkef_DUD} },\n",
    "              'DEKOIS': {'Dk_sc': {'X': X_dksc_DEKOIS,  'y': y_dksc_DEKOIS}, \n",
    "                       'Dk_lef':  {'X': X_dkef_DEKOIS,  'y': y_dkef_DEKOIS} },\n",
    "             }\n",
    "\n",
    "def get_docking_scores_dict():\n",
    "    return scores_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best conformation from docking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_conformation(db_name, score_type, scores_dic = scores_dic):\n",
    "    _y_true = scores_dic[db_name][score_type]['y']\n",
    "    # Calculating the roc_acu_value\n",
    "    auc_scores = scores_dic[db_name][score_type]['X'].apply(\n",
    "        lambda x: roc_auc_score(y_true=_y_true, y_score = -1 * x), axis = 0)\n",
    "\n",
    "    name = auc_scores.idxmax()\n",
    "    value = auc_scores.max()\n",
    "    \n",
    "    return (name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dud, bc_dud_auc = get_best_conformation('DUD', 'Dk_sc')\n",
    "bc_dekois, bc_dekois_auc = get_best_conformation('DEKOIS', 'Dk_sc')\n",
    "bc_csar, bc_csar_auc = get_best_conformation('CSAR', 'Dk_sc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_results(results_dic, test_keys, ax, legend_position = 'lower right', plot_refs = True):\n",
    "    for test in test_keys:\n",
    "        for key in results_dic.keys():\n",
    "            sns.lineplot(x = 'variable', y = 'value', ax = ax, #Random\n",
    "                 data = results_dic[key][test].melt(), ci = 'sd', label = F'{test}: {key}')\n",
    "\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    if plot_refs:\n",
    "        # Uses the best conformation auc with vinardo in DUD and DEKOIS\n",
    "        _plot_refs(ax)\n",
    "    ax.legend(handles = handles[:], loc = legend_position)\n",
    "    ax.set_ylim((0.4, 0.9))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_refs(ax):\n",
    "    ref_dud = plt.axhline(bc_dud_auc, ls='--', label = 'DUD: Best. conf Vinardo', c = 'blue')\n",
    "    ref_dekois = plt.axhline(bc_dekois_auc, ls='--', label = 'DEKOIS: Best. conf Vinardo', c = 'magenta')\n",
    "    leg = plt.legend(handles = [ref_dud, ref_dekois], loc='upper right')\n",
    "    ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the MDS resutls to be used for clustering\n",
    "import pickle\n",
    "path_mds_obj = os.path.join('..', 'data', 'trajectory_analysis', 'cMDS_Pisani_402_obj.pyobj')\n",
    "with open(path_mds_obj, 'rb') as f:\n",
    "    mds_pisani_402 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, estimator, estimator_hyparams = None,\n",
    "               scoring = 'roc_auc', k_folds = 5, standarize = True, \n",
    "                rfe = False, rfe_n_features = 2,\n",
    "               split_train = False, test_size = 0.2, random_state = 1, **kwargs):\n",
    "    '''If desire, the original train set cab be splitted. Just in case of been useful'''\n",
    "    if split_train:\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train, y_train, test_size = test_size,\n",
    "                         stratify = y_train, random_state = random_state)\n",
    "    \n",
    "    '''\n",
    "     1) Pipeline is created, and will perform StandarScaler. More steps can be added later.\n",
    "     # pipe args is a list of tuples initialized with one element; the estimator.\n",
    "     # If standarize = true it adds StandardScaler at the bigining of the pipe.\n",
    "    '''\n",
    "    pipe_args = [(\"estimator\", estimator(**kwargs))]\n",
    "    \n",
    "    '''Recursive Feature Elimination'''\n",
    "    if rfe:\n",
    "        pipe_args.insert(0, (\"rfe\", RFE(estimator(**kwargs), \n",
    "                                        n_features_to_select = rfe_n_features,\n",
    "                                        step = 1)))\n",
    "    if standarize:\n",
    "        pipe_args.insert(0, (\"scaler\", StandardScaler()))\n",
    "    pipe = Pipeline(pipe_args)\n",
    "    '''\n",
    "    2) The specific hyperparameters of the selected stimator are given, \n",
    "    we will parse them to the gridSearch instance.\n",
    "    '''\n",
    "    if estimator_hyparams != None:\n",
    "        params = {}\n",
    "        for key, value in estimator_hyparams.items():\n",
    "            params['estimator__' + key] = value\n",
    "        '''\n",
    "        3) Grid search cross validation for turning the optimal parameters, \n",
    "        it takes the pipeline object. GridSearch performs k-fold cross validation, \n",
    "        and uses the given scoring method to validate each set.\n",
    "\n",
    "        '''\n",
    "        grid = GridSearchCV(estimator = pipe, param_grid = params, \n",
    "                            cv = k_folds, scoring = scoring,\n",
    "                            n_jobs = 6, refit=True)\n",
    "        estimator = grid\n",
    "        \n",
    "    else:\n",
    "        '''Additionaly, if estimator_hyparams is None, Grid search is avoided.'''\n",
    "        estimator = pipe\n",
    "        \n",
    "    '''SVC training through GridSearch object'''\n",
    "    estimator.fit(X_train, y_train)\n",
    "    '''Return the trained estimator (an instance from GridSearchCV or Pipeline)'''\n",
    "    #final_model = estimator.best_estimator_ if estimator_hyparams != None else estimator\n",
    "    return(estimator)\n",
    "\n",
    "def eval_model(model, X_test, y_test, return_proba = True):\n",
    "    '''\n",
    "    1) Predictions and evaluation on the Test set\n",
    "    - Scaling and prediction of X_train using the best model found by grid\n",
    "    '''\n",
    "    if return_proba:\n",
    "        y_prob  = model.predict_proba(X_test)[:,1] # Predicted prob values for X_test\n",
    "        y_hat = y_prob\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "        y_hat = y_score\n",
    "    y_pred  = model.predict(X_test) # predicted values\n",
    "    '''Returns the y_score values and the lnear_SVC object'''\n",
    "    return(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(train_key, list_of_scores, estimator,\n",
    "                  scores_dic, selected_features = None, **kwargs):\n",
    "    ''''''\n",
    "    trained_models = {}\n",
    "    for score in list_of_scores:\n",
    "        if selected_features is None:\n",
    "            X_train = scores_dic[train_key][score]['X']\n",
    "        else:\n",
    "            X_train = scores_dic[train_key][score]['X'][selected_features]\n",
    "        y_train = scores_dic[train_key][score]['y']\n",
    "        name = F'{score}'\n",
    "        trained_models[name] = train_model(X_train, y_train, estimator, **kwargs)\n",
    "    return(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_wrapper(trained_model, test_keys, list_of_scores,\n",
    "                  scores_dic, selected_features = None, return_proba = True, **kwargs):\n",
    "    model_results = {}\n",
    "    for test_key in test_keys:\n",
    "        y_preds = {}\n",
    "        for score in list_of_scores:\n",
    "            if selected_features is None:\n",
    "                X_test  = scores_dic[test_key][score]['X']\n",
    "            else: \n",
    "                X_test  = scores_dic[test_key][score]['X'][selected_features]\n",
    "            y_test  = scores_dic[test_key][score]['y']\n",
    "            name = F'{test_key}-{score}'\n",
    "            y_preds[name] = eval_model(trained_model[score], X_test, y_test, return_proba = return_proba)\n",
    "        # Invoke PlotMetric Class\n",
    "        model_results[test_key] = PlotMetric(y_true = y_test, y_pred_dict = y_preds, **kwargs)\n",
    "    return(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_results(title, predictions, train_key, plot_rankings = True, plot_nef = False):\n",
    "    n_rows = 2 if plot_nef else 1\n",
    "    plt.figure(figsize=(14, 7*n_rows))\n",
    "    #plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for i, test_set in enumerate(test_keys):\n",
    "        plt.subplot(F'{n_rows}2{i+1}')\n",
    "        predictions[test_set].plot_roc_auc(F'{title}:\\n{train_key} train, {test_set} test', \n",
    "                                     show_by_itself = False, fontsize = 'x-small')\n",
    "        if plot_nef:\n",
    "            plt.subplot(F'{n_rows}2{i+3}')\n",
    "            predictions[test_set].plot_ef_auc('', method = 'normalized', max_chi = 0.1, \n",
    "                                         show_by_itself = False, fontsize = 'x-small')\n",
    "    plt.show()\n",
    "    if plot_rankings:\n",
    "        for test_set in test_keys:\n",
    "            predictions[test_set].plot_actives_distribution(max_position_to_plot=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for random picking n features and perform SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_confs_picking(scores_dic,\n",
    "                             score_type,\n",
    "                             train_key, \n",
    "                             test_keys,\n",
    "                             model, \n",
    "                             n_features,\n",
    "                             n_reps = 50,\n",
    "                             metric = 'roc_auc',\n",
    "                             **kwargs,\n",
    "                            ):\n",
    "    # Creates the dictionary of results to be filled and returned\n",
    "    results_dict = {}\n",
    "    for test in test_keys:\n",
    "        results_dict[test] = np.zeros(n_reps)\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        '''Performs the random selection'''\n",
    "        random_features = np.random.choice(a = range(0, 402), size = n_features, replace=False)\n",
    "        \n",
    "        random_features = np.sort(random_features)\n",
    "        features = scores_dic[train_key][score_type]['X'].columns[random_features]\n",
    "        \n",
    "        model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = model,\n",
    "                               selected_features = features,\n",
    "                               **kwargs)\n",
    "        \n",
    "        model_pred = eval_wrapper(trained_model = model_train, \n",
    "                            test_keys = test_keys, list_of_scores = [score_type], \n",
    "                            selected_features = features,\n",
    "                            scores_dic = scores_dic, decreasing = False)\n",
    "        \n",
    "        for test in test_keys:\n",
    "            metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "            results_dict[test][rep] = metric_value\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtion to perform kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_medoids_idx(X, n_dims = 2, n_clusters = 10, random_state = None):\n",
    "    _X = X[:n_dims].T\n",
    "\n",
    "    kmc = KMeans(n_clusters = n_clusters, random_state = random_state)\n",
    "    kmc.fit( _X )\n",
    "    \n",
    "    # Get the distances from the centroids\n",
    "    _y = kmc.labels_\n",
    "    medoids_idx = []\n",
    "    for label in sorted(np.unique(_y)):\n",
    "        label_indices = np.where(_y == label)[0]\n",
    "        # Get the distance from the centroid\n",
    "        # centroid\n",
    "        centroid = kmc.cluster_centers_[label]\n",
    "        # Points inside the cluster\n",
    "        _X_points = _X[label_indices]\n",
    "        # Compute the distances\n",
    "        _X_distances = ((_X_points - centroid)**2).sum(axis = 1)\n",
    "        # Get the closest point\n",
    "        medoid_idx = label_indices[np.argmin(_X_distances)]\n",
    "        medoids_idx.append(medoid_idx)\n",
    "\n",
    "    return medoids_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFE_wrapper(ml_model, model_name, scores_dic, train_key, test_keys,\n",
    "                      min_confs = 1, max_confs = 402, interval = 1, \n",
    "                      score_type = 'Dk_sc', metric = 'roc_auc', \n",
    "                      file_path = '../data/ml_evaluations/ml_models/', file_sufix = '',\n",
    "                      verbose = False, \n",
    "                      train_by_itself = True, model_for_RFE = None,\n",
    "                      rfe_model_hyparms = None,\n",
    "                      **kwargs):\n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(test_keys) + \\\n",
    "            F\"_{model_name}_RFE_{score_type.replace('_', '')}\" + \\\n",
    "            F'_range_{min_confs}_{max_confs}' + file_sufix + '.obj'\n",
    "    list_of_fetures_file_name =  file_name + '_RFE-Features_.npy'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            results_df = pickle.load(f)\n",
    "        # Also tries to load the list of features used at each step\n",
    "        try:\n",
    "            array_of_features = np.load(list_of_fetures_file_name)\n",
    "        except FileNotFoundError:\n",
    "            array_of_features = None\n",
    "    else:\n",
    "        results_dic = {}\n",
    "        array_of_features = np.zeros((max_confs - min_confs + 1,\n",
    "                                      max_confs))\n",
    "        for test in test_keys:\n",
    "            results_dic[test] = np.zeros((1, max_confs - min_confs + 1))\n",
    "\n",
    "        for k in range(min_confs, max_confs + 1, interval):\n",
    "            if verbose and ((max_confs - min_confs + 1) % 10) == 0:\n",
    "                print(F'Step: {k - min_confs}')\n",
    "                \n",
    "                \n",
    "            if not train_by_itself:\n",
    "                assert model_for_RFE != None, 'model_for_RFE must be a classifier with' + \\\n",
    "                '\"coef_\" or \"feature_importances_\" attributes'\n",
    "                # Trains with the selected model for RFE\n",
    "                RFE_train = train_wrapper(train_key = train_key, \n",
    "                               list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = model_for_RFE,\n",
    "                               rfe = True, rfe_n_features = k,\n",
    "                               **rfe_model_hyparms)\n",
    "                # Get the features\n",
    "                _selected_features_mask = RFE_train[score_type]['rfe'].support_\n",
    "                # Add the features to the list to track the fatures used\n",
    "                array_of_features[k - min_confs, :] = _selected_features_mask\n",
    "                \n",
    "                _selected_features = scores_dic[train_key][score_type]['X'].iloc[:,_selected_features_mask].columns\n",
    "                # Now, train the main estimator with the selected features\n",
    "                model_train = train_wrapper(train_key = train_key, \n",
    "                               list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = ml_model,\n",
    "                               rfe = True, rfe_n_features = k,\n",
    "                               selected_features = _selected_features,\n",
    "                               **kwargs)\n",
    "                \n",
    "                model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                selected_features = _selected_features,\n",
    "                                scores_dic = scores_dic, decreasing = False)\n",
    "                \n",
    "            else:\n",
    "                model_train = train_wrapper(train_key = train_key, \n",
    "                               list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = ml_model,\n",
    "                               rfe = True, rfe_n_features = k,\n",
    "                               **kwargs)\n",
    "                \n",
    "                # Get the features\n",
    "                _selected_features_mask = model_train[score_type]['rfe'].support_\n",
    "                # Add the features to the list to track the fatures used\n",
    "                array_of_features[k - min_confs, :] = _selected_features_mask\n",
    "\n",
    "                model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                scores_dic = scores_dic, decreasing = False)\n",
    "            # Get the auc values\n",
    "            for test in test_keys:\n",
    "                metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "                results_dic[test][0, k - min_confs] = metric_value\n",
    "                \n",
    "            # Create the dataframe\n",
    "            columns_ = [k for k in range(min_confs, max_confs + 1)]\n",
    "\n",
    "            results_df = {}\n",
    "            for test in test_keys:\n",
    "                df_ = pd.DataFrame(data = results_dic[test], columns = columns_)\n",
    "                results_df[test] = df_\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(results_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Saves the list of features at each step '_FEATURES_' + file_name\n",
    "        np.save(list_of_fetures_file_name, array_of_features.astype(bool))\n",
    "    return results_df, array_of_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for ML features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_picking_ML(ml_model, model_name, scores_dic, train_key, test_keys,\n",
    "                      df_precomputed_medoids = None,\n",
    "                      min_confs = 1, max_confs = 402, interval = 1, \n",
    "                      score_type = 'Dk_sc', metric = 'roc_auc', \n",
    "                      file_path = '../data/ml_evaluations/', file_sufix = '',\n",
    "                      **kwargs):\n",
    "    \n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(test_keys) + \\\n",
    "            F\"_{model_name}_kmeans_{score_type.replace('_', '')}\" + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            results_df = pickle.load(f)\n",
    "    else:        \n",
    "        results_dic = {}\n",
    "        for test in test_keys:\n",
    "            results_dic[test] = np.zeros((1, max_confs - min_confs + 1))\n",
    "\n",
    "        for k in range(min_confs, max_confs + 1, interval):\n",
    "            if df_precomputed_medoids is None:\n",
    "                medoids_idx = get_medoids_idx(data_to_clust, n_dims = n_dims, n_clusters = k,\n",
    "                                         random_state = random_state)\n",
    "            else: \n",
    "                medoids_idx = df_precomputed_medoids.iloc[k - 1].values[0] \n",
    "                # Get the list of conformations from precomputed kmeans\n",
    "            features = scores_dic[train_key][score_type]['X'].columns[medoids_idx]\n",
    "            model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                                   scores_dic = scores_dic, estimator = ml_model,\n",
    "                                   selected_features = features,\n",
    "                                   **kwargs)\n",
    "            # Perfrom the testing\n",
    "            model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                selected_features = features,\n",
    "                                scores_dic = scores_dic, decreasing = False)\n",
    "            # Get the auc values\n",
    "            for test in test_keys:\n",
    "                metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "                results_dic[test][0, k - min_confs] = metric_value\n",
    "        # Create the dataframe\n",
    "        columns_ = [k for k in range(min_confs, max_confs + 1)]\n",
    "\n",
    "        results_df = {}\n",
    "        for test in test_keys:\n",
    "            df_ = pd.DataFrame(data = results_dic[test], columns = columns_)\n",
    "            results_df[test] = df_\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(results_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_picking_ML(ml_model, model_name, scores_dic, train_key, test_keys,\n",
    "                      min_confs = 1, max_confs = 402, interval = 1, n_reps = 30,  \n",
    "                      score_type = 'Dk_sc', metric = 'roc_auc',\n",
    "                      file_path = '../data/ml_evaluations/', file_sufix = '',\n",
    "                      **kwargs):\n",
    "    \n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(test_keys) + \\\n",
    "            F\"_{model_name}_random_{score_type.replace('_', '')}\" + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            results_df = pickle.load(f)\n",
    "    else:        \n",
    "        results_dic = {}\n",
    "        for test in test_keys:\n",
    "            results_dic[test] = np.zeros((n_reps, (max_confs - min_confs + 1)))\n",
    "\n",
    "        for m in range(min_confs, max_confs + 1, interval):\n",
    "\n",
    "            for rep in range(n_reps):\n",
    "                # get the n conformations randomly, no repetition\n",
    "                random_features = np.random.choice(a = range(0, max_confs), \n",
    "                                                   size = m, replace = False)\n",
    "                # Select the conformations\n",
    "                features = scores_dic[train_key][score_type]['X'].columns[random_features]\n",
    "                # Perform the training\n",
    "                model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                                       scores_dic = scores_dic, estimator = ml_model,\n",
    "                                       selected_features = features,\n",
    "                                       **kwargs)\n",
    "                # Perfrom the testing\n",
    "                model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                    test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                    selected_features = features,\n",
    "                                    scores_dic = scores_dic, decreasing = False)\n",
    "                # Get the auc values\n",
    "                for test in test_keys:\n",
    "                    metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "                    results_dic[test][rep, m - 1] = metric_value\n",
    "        # Create the dataframe\n",
    "        index_ = [F'rep_{n}' for n in range(n_reps)]\n",
    "        columns_ = [m for m in range(1, max_confs + 1)]\n",
    "\n",
    "        results_df = {}\n",
    "        for test in test_keys:\n",
    "            df_ = pd.DataFrame(data = results_dic[test], index = index_, columns = columns_)\n",
    "            results_df[test] = df_\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(results_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for consensus scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random picking conformations for consensus scoring\n",
    "def random_picking_consensus(f, df_X, df_y, min_confs, max_confs, interval, n_reps, **kwargs):\n",
    "    '''Performs a consensus scoring n_rep times for a range of min_confs to max_confs'''\n",
    "    results_dic = {}\n",
    "    for n in range(min_confs, max_confs + 1, interval):\n",
    "        auc_array = np.zeros(n_reps)\n",
    "        for rep in range(n_reps):\n",
    "            # get the n conformations randomly, no repetition\n",
    "            random_features = np.random.choice(a = range(0, max_confs), size = n, replace = False)\n",
    "            _X = df_X.iloc[:, random_features] # Filter the dataframe\n",
    "            # Assume the rrank by number as the y_pred \n",
    "            y_pred = f(df=_X, **kwargs) \n",
    "\n",
    "            # Get the roc auc\n",
    "            auc = roc_auc_score(df_y, y_pred)\n",
    "            auc_array[rep] = auc\n",
    "        results_dic[n] = auc_array\n",
    "    return pd.DataFrame(results_dic)\n",
    "\n",
    "# K-means picking conformations for consensus scoring\n",
    "def kmeans_picking_consensus(f, df_X, df_y, min_confs, max_confs, interval, \n",
    "                             data_to_clust = mds_pisani_402[0], \n",
    "                             df_precomputed_medoids = None, # Should be a dictionary {'pisani': prep_pisani}\n",
    "                             n_dims = 2,\n",
    "                             random_state = 0, **kwargs):\n",
    "    '''Performs a consensus scoring giving k selected features thorugh k-means medoids selection'''\n",
    "    results_dic = {}\n",
    "    for key, precomputed in df_precomputed_medoids.items():\n",
    "        auc_array = np.zeros(max_confs)\n",
    "        \n",
    "        for k in range(min_confs, max_confs + 1, interval):\n",
    "            # TODO: data_to_clust right now can only manage only one MDS subspace\n",
    "            if df_precomputed_medoids is None:\n",
    "                medoids_idx = get_medoids_idx(data_to_clust, n_dims = n_dims, n_clusters = k,\n",
    "                                         random_state = random_state)\n",
    "            else:\n",
    "                medoids_idx = precomputed.iloc[k - 1].values[0] # Get the list of conformations from precomputed kmeans\n",
    "                features = df_X.columns[medoids_idx]\n",
    "                _X = df_X[features] # Filter the dataframe\n",
    "                # Assume the rrank by number as the y_pred \n",
    "                y_pred = f(_X, **kwargs)\n",
    "                # Get the roc auc\n",
    "                auc = roc_auc_score(df_y, y_pred)\n",
    "                auc_array[k - 1] = auc\n",
    "        results_dic[key] = auc_array\n",
    "    return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "def consensus_wrapper(func, scores_dic, score_types, dataset_names, consensus_name, \n",
    "                             file_path = '../data/ml_evaluations/consensus_scoring/', \n",
    "                             file_sufix = '', method = 'random',\n",
    "                             min_confs = 1, max_confs = 402, interval = 1, n_reps = 50, \n",
    "                             overwrite = False,\n",
    "                             **kwargs):\n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(dataset_names) + \\\n",
    "            F'_{method}_Cons_{consensus_name}' + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    _results_dict = {}\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            _results_dict = pickle.load(f)\n",
    "    if not os.path.isfile(file_name) or overwrite:\n",
    "        # If overwrite or not\n",
    "        for dataset in dataset_names:\n",
    "            for score in score_types:\n",
    "                # Creates the key name\n",
    "                key_name = F'{dataset}_{score.replace(\"_\", \"\")}_{method}_Cons_{consensus_name}'\n",
    "                # Determines if the key exist\n",
    "                if key_name in _results_dict.keys():\n",
    "                    #print(F'{key_name} already exists!')\n",
    "                    continue\n",
    "                df_X = scores_dic[dataset][score]['X']\n",
    "                df_y = scores_dic[dataset][score]['y']\n",
    "                if method == 'random':\n",
    "                    df_aucs = random_picking_consensus(func, df_X, df_y, \n",
    "                                         min_confs, max_confs, interval, n_reps, **kwargs)\n",
    "                elif method == 'kmeans':\n",
    "                    df_aucs = kmeans_picking_consensus(func, df_X, df_y, \n",
    "                                     min_confs, max_confs, interval, **kwargs)\n",
    "                else:\n",
    "                    print('Wrong method for conformations selection (\"random\", \"kmeans\").')\n",
    "                # Add to the dictionary of results\n",
    "                _results_dict[key_name] = df_aucs\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(_results_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return(_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 3, 5]),)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([True, False, False, True, False, True])\n",
    "np.where(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
