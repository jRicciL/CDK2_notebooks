{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_results(path, column_to_drop, active_label='Active'):\n",
    "    '''Function to load VS results from path file.\n",
    "    It returns X and y ndarrays.'''\n",
    "    path_file = os.path.join(*path.split('/'))\n",
    "    df_results = pd.read_csv(path_file, index_col=0)\n",
    "    y_ = pd.Series(df_results[column_to_drop] == active_label, dtype = int) # Setting y_true\n",
    "    X_ = df_results.drop([column_to_drop], axis = 1) # Setting X\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, estimator, estimator_hyparams = None,\n",
    "               scoring = 'roc_auc', k_folds = 5, standarize = True, \n",
    "               split_train = False, test_size = 0.2, random_state = 1, **kwargs):\n",
    "    '''If desire, the original train set cab be splitted. Just in case of been useful'''\n",
    "    if split_train:\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train, y_train, test_size = test_size,\n",
    "                         stratify = y_train, random_state = random_state)\n",
    "    \n",
    "    '''\n",
    "     1) Pipeline is created, and will perform StandarScaler. More steps can be added later.\n",
    "     # pipe args is a list of tuples initialized with one element; the estimator.\n",
    "     # If standarize = true it adds StandardScaler at the bigining of the pipe.\n",
    "    '''\n",
    "    pipe_args = [(\"estimator\", estimator(**kwargs))]\n",
    "    if standarize:\n",
    "        pipe_args.insert(0, (\"scaler\", StandardScaler()))\n",
    "    pipe = Pipeline(pipe_args)\n",
    "    '''\n",
    "    2) The specific hyperparameters of the selected stimator are given, \n",
    "    we will parse them to the gridSearch instance.\n",
    "    '''\n",
    "    if estimator_hyparams != None:\n",
    "        params = {}\n",
    "        for key, value in estimator_hyparams.items():\n",
    "            params['estimator__' + key] = value\n",
    "        '''\n",
    "        3) Grid search cross validation for turning the optimal parameters, \n",
    "        it takes the pipeline object. GridSearch performs k-fold cross validation, \n",
    "        and uses the given scoring method to validate each set.\n",
    "\n",
    "        '''\n",
    "        grid = GridSearchCV(estimator = pipe, param_grid = params, \n",
    "                            cv = k_folds, scoring = scoring,\n",
    "                            n_jobs = 6, refit=True)\n",
    "        estimator = grid\n",
    "        \n",
    "    else:\n",
    "        '''Additionaly, if estimator_hyparams is None, Grid search is avoided.'''\n",
    "        estimator = pipe\n",
    "        \n",
    "    '''SVC training through GridSearch object'''\n",
    "    estimator.fit(X_train, y_train)\n",
    "    '''Return the trained estimator (an instance from GridSearchCV or Pipeline)'''\n",
    "    #final_model = estimator.best_estimator_ if estimator_hyparams != None else estimator\n",
    "    return(estimator)\n",
    "\n",
    "def eval_model(model, X_test, y_test, return_proba = True):\n",
    "    '''\n",
    "    1) Predictions and evaluation on the Test set\n",
    "    - Scaling and prediction of X_train using the best model found by grid\n",
    "    '''\n",
    "    if return_proba:\n",
    "        y_prob  = model.predict_proba(X_test)[:,1] # Predicted prob values for X_test\n",
    "        y_hat = y_prob\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "        y_hat = y_score\n",
    "    y_pred  = model.predict(X_test) # predicted values\n",
    "    '''Returns the y_score values and the lnear_SVC object'''\n",
    "    return(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(train_key, list_of_scores, estimator,\n",
    "                  scores_dic, selected_features = None, **kwargs):\n",
    "    ''''''\n",
    "    trained_models = {}\n",
    "    for score in list_of_scores:\n",
    "        if selected_features is None:\n",
    "            X_train = scores_dic[train_key][score]['X']\n",
    "        else:\n",
    "            X_train = scores_dic[train_key][score]['X'][selected_features]\n",
    "        y_train = scores_dic[train_key][score]['y']\n",
    "        name = F'{score}'\n",
    "        trained_models[name] = train_model(X_train, y_train, estimator, **kwargs)\n",
    "    return(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_wrapper(trained_model, test_keys, list_of_scores,\n",
    "                  scores_dic, selected_features = None, return_proba = True, **kwargs):\n",
    "    model_results = {}\n",
    "    for test_key in test_keys:\n",
    "        y_preds = {}\n",
    "        for score in list_of_scores:\n",
    "            if selected_features is None:\n",
    "                X_test  = scores_dic[test_key][score]['X']\n",
    "            else: \n",
    "                X_test  = scores_dic[test_key][score]['X'][selected_features]\n",
    "            y_test  = scores_dic[test_key][score]['y']\n",
    "            name = F'{test_key}-{score}'\n",
    "            y_preds[name] = eval_model(trained_model[score], X_test, y_test, return_proba = return_proba)\n",
    "        # Invoke PlotMetric Class\n",
    "        model_results[test_key] = PlotMetric(y_true = y_test, y_pred_dict = y_preds, **kwargs)\n",
    "    return(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_results(title, predictions, train_key, plot_rankings = True, plot_nef = False):\n",
    "    n_rows = 2 if plot_nef else 1\n",
    "    plt.figure(figsize=(14, 7*n_rows))\n",
    "    #plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for i, test_set in enumerate(test_keys):\n",
    "        plt.subplot(F'{n_rows}2{i+1}')\n",
    "        predictions[test_set].plot_roc_auc(F'{title}:\\n{train_key} train, {test_set} test', \n",
    "                                     show_by_itself = False, fontsize = 'x-small')\n",
    "        if plot_nef:\n",
    "            plt.subplot(F'{n_rows}2{i+3}')\n",
    "            predictions[test_set].plot_ef_auc('', method = 'normalized', max_chi = 0.1, \n",
    "                                         show_by_itself = False, fontsize = 'x-small')\n",
    "    plt.show()\n",
    "    if plot_rankings:\n",
    "        for test_set in test_keys:\n",
    "            predictions[test_set].plot_actives_distribution(max_position_to_plot=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for random picking n features and perform SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_confs_picking(scores_dic,\n",
    "                             score_type,\n",
    "                             train_key, \n",
    "                             test_keys,\n",
    "                             model, \n",
    "                             n_features,\n",
    "                             n_reps = 50,\n",
    "                             metric = 'roc_auc',\n",
    "                             **kwargs,\n",
    "                            ):\n",
    "    # Creates the dictionary of results to be filled and returned\n",
    "    results_dict = {}\n",
    "    for test in test_keys:\n",
    "        results_dict[test] = np.zeros(n_reps)\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        '''Performs the random selection'''\n",
    "        random_features = np.random.choice(a = range(0, 402), size = n_features, replace=False)\n",
    "        \n",
    "        random_features = np.sort(random_features)\n",
    "        features = scores_dic[train_key][score_type]['X'].columns[random_features]\n",
    "        \n",
    "        model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = model,\n",
    "                               selected_features = features,\n",
    "                               **kwargs)\n",
    "        \n",
    "        model_pred = eval_wrapper(trained_model = model_train, \n",
    "                            test_keys = test_keys, list_of_scores = [score_type], \n",
    "                            selected_features = features,\n",
    "                            scores_dic = scores_dic, decreasing = False)\n",
    "        \n",
    "        for test in test_keys:\n",
    "            metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "            results_dict[test][rep] = metric_value\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtion to perform kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_medoids_idx(X, n_dims = 2, n_clusters = 10, random_state = None):\n",
    "    _X = X[:fd].T\n",
    "\n",
    "    kmc = KMeans(n_clusters = n_clusters, random_state = random_state)\n",
    "    kmc.fit( _X )\n",
    "    \n",
    "    # Get the distances from the centroids\n",
    "    _y = kmc.labels_\n",
    "    medoids_idx = []\n",
    "    for label in sorted(np.unique(_y)):\n",
    "        label_indices = np.where(_y == label)[0]\n",
    "        # Get the distance from the centroid\n",
    "        # centroid\n",
    "        centroid = kmc.cluster_centers_[label]\n",
    "        # Points inside the cluster\n",
    "        _X_points = _X[label_indices]\n",
    "        # Compute the distances\n",
    "        _X_distances = ((_X_points - centroid)**2).sum(axis = 1)\n",
    "        # Get the closest point\n",
    "        medoid_idx = label_indices[np.argmin(_X_distances)]\n",
    "        medoids_idx.append(medoid_idx)\n",
    "\n",
    "    return medoids_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for ML features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_picking_ML(ml_model, model_name, scores_dic, train_key, test_keys,\n",
    "                      df_precomputed_medoids = None,\n",
    "                      min_confs = 1, max_confs = 402, interval = 1, \n",
    "                      score_type = 'Dk_sc', metric = 'roc_auc', \n",
    "                      file_path = '../data/ml_evaluations/', file_sufix = '',\n",
    "                      **kwargs):\n",
    "    \n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(test_keys) + \\\n",
    "            F\"_{model_name}_kmeans_{score_type.replace('_', '')}\" + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            results_df = pickle.load(f)\n",
    "    else:        \n",
    "        results_dic = {}\n",
    "        for test in test_keys:\n",
    "            results_dic[test] = np.zeros((1, max_confs - min_confs + 1))\n",
    "\n",
    "        for k in range(min_confs, max_confs + 1, interval):\n",
    "            if df_precomputed_medoids is None:\n",
    "                medoids_idx = get_medoids_idx(data_to_clust, n_dims = n_dims, n_clusters = k,\n",
    "                                         random_state = random_state)\n",
    "            else: \n",
    "                medoids_idx = df_precomputed_medoids.iloc[k - 1].values[0] \n",
    "                # Get the list of conformations from precomputed kmeans\n",
    "            features = scores_dic[train_key][score_type]['X'].columns[medoids_idx]\n",
    "            model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                                   scores_dic = scores_dic, estimator = ml_model,\n",
    "                                   selected_features = features,\n",
    "                                   **kwargs)\n",
    "            # Perfrom the testing\n",
    "            model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                selected_features = features,\n",
    "                                scores_dic = scores_dic, decreasing = False)\n",
    "            # Get the auc values\n",
    "            for test in test_keys:\n",
    "                metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "                results_dic[test][0, k - 1] = metric_value\n",
    "        # Create the dataframe\n",
    "        columns_ = [k for k in range(1, max_confs + 1)]\n",
    "\n",
    "        results_df = {}\n",
    "        for test in test_keys:\n",
    "            df_ = pd.DataFrame(data = results_dic[test], columns = columns_)\n",
    "            results_df[test] = df_\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(results_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_picking_ML(ml_model, model_name, scores_dic, train_key, test_keys,\n",
    "                      min_confs = 1, max_confs = 402, interval = 1, n_reps = 30,  \n",
    "                      score_type = 'Dk_sc', metric = 'roc_auc',\n",
    "                      file_path = '../data/ml_evaluations/', file_sufix = '',\n",
    "                      **kwargs):\n",
    "    \n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(test_keys) + \\\n",
    "            F\"_{model_name}_random_{score_type.replace('_', '')}\" + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            results_df = pickle.load(f)\n",
    "    else:        \n",
    "        results_dic = {}\n",
    "        for test in test_keys:\n",
    "            results_dic[test] = np.zeros((n_reps, (max_confs - min_confs + 1)))\n",
    "\n",
    "        for m in range(min_confs, max_confs + 1, interval):\n",
    "\n",
    "            for rep in range(n_reps):\n",
    "                # get the n conformations randomly, no repetition\n",
    "                random_features = np.random.choice(a = range(0, max_confs), \n",
    "                                                   size = m, replace = False)\n",
    "                # Select the conformations\n",
    "                features = scores_dic[train_key][score_type]['X'].columns[random_features]\n",
    "                # Perform the training\n",
    "                model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                                       scores_dic = scores_dic, estimator = ml_model,\n",
    "                                       selected_features = features,\n",
    "                                       **kwargs)\n",
    "                # Perfrom the testing\n",
    "                model_pred = eval_wrapper(trained_model = model_train, \n",
    "                                    test_keys = test_keys, list_of_scores = [score_type], \n",
    "                                    selected_features = features,\n",
    "                                    scores_dic = scores_dic, decreasing = False)\n",
    "                # Get the auc values\n",
    "                for test in test_keys:\n",
    "                    metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "                    results_dic[test][rep, m - 1] = metric_value\n",
    "        # Create the dataframe\n",
    "        index_ = [F'rep_{n}' for n in range(n_reps)]\n",
    "        columns_ = [m for m in range(1, max_confs + 1)]\n",
    "\n",
    "        results_df = {}\n",
    "        for test in test_keys:\n",
    "            df_ = pd.DataFrame(data = results_dic[test], index = index_, columns = columns_)\n",
    "            results_df[test] = df_\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(results_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for consensus scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random picking conformations for consensus scoring\n",
    "def random_picking_consensus(f, df_X, df_y, min_confs, max_confs, interval, n_reps, **kwargs):\n",
    "    '''Performs a consensus scoring n_rep times for a range of min_confs to max_confs'''\n",
    "    results_dic = {}\n",
    "    for n in range(min_confs, max_confs + 1, interval):\n",
    "        auc_array = np.zeros(n_reps)\n",
    "        for rep in range(n_reps):\n",
    "            # get the n conformations randomly, no repetition\n",
    "            random_features = np.random.choice(a = range(0, max_confs), size = n, replace = False)\n",
    "            _X = df_X.iloc[:, random_features] # Filter the dataframe\n",
    "            # Assume the rrank by number as the y_pred \n",
    "            y_pred = f(df=_X, **kwargs) \n",
    "\n",
    "            # Get the roc auc\n",
    "            auc = roc_auc_score(df_y, y_pred)\n",
    "            auc_array[rep] = auc\n",
    "        results_dic[n] = auc_array\n",
    "    return pd.DataFrame(results_dic)\n",
    "\n",
    "# K-means picking conformations for consensus scoring\n",
    "def kmeans_picking_consensus(f, df_X, df_y, min_confs, max_confs, interval, \n",
    "                             df_precomputed_medoids = None,\n",
    "                             data_to_clust = mds_pisani_402[0], n_dims = 2,\n",
    "                             random_state = 0, **kwargs):\n",
    "    '''Performs a consensus scoring giving k selected features thorugh k-means medoids selection'''\n",
    "    auc_array = np.zeros(max_confs)\n",
    "    for k in range(min_confs, max_confs + 1, interval):\n",
    "        if df_precomputed_medoids is None:\n",
    "            medoids_idx = get_medoids_idx(data_to_clust, n_dims = n_dims, n_clusters = k,\n",
    "                                     random_state = random_state)\n",
    "        else: \n",
    "            medoids_idx = df_precomputed_medoids.iloc[k - 1].values[0] # Get the list of conformations from precomputed kmeans\n",
    "        features = scores_dic[train_key][score_type]['X'].columns[medoids_idx]\n",
    "        _X = df_X[features] # Filter the dataframe\n",
    "        # Assume the rrank by number as the y_pred \n",
    "        y_pred = f(_X, **kwargs)\n",
    "        # Get the roc auc\n",
    "        auc = roc_auc_score(df_y, y_pred)\n",
    "        auc_array[k - 1] = auc\n",
    "    return auc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "def consensus_wrapper(func, scores_dic, score_types, dataset_names, consensus_name, \n",
    "                             file_path = '../data/ml_evaluations/', file_sufix = '', method = 'random',\n",
    "                             min_confs = 1, max_confs = 402, interval = 1, n_reps = 50, \n",
    "                             **kwargs):\n",
    "    # File name\n",
    "    file_name = file_path + '_'.join(dataset_names) + \\\n",
    "            F'_{method}_Cons_{consensus_name}' + file_sufix + '.obj'\n",
    "    # Check if the file exist\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            _results_dict = pickle.load(f)\n",
    "    else:\n",
    "        _results_dict = {}\n",
    "        for dataset in dataset_names:\n",
    "            for score in score_types:\n",
    "                key_name = F'{dataset}_{score.replace(\"_\", \"\")}_{method}_Cons_{consensus_name}'\n",
    "                df_X = scores_dic[dataset][score]['X']\n",
    "                df_y = scores_dic[dataset][score]['y']\n",
    "                if method == 'random':\n",
    "                    df_aucs = random_picking_consensus(func, df_X, df_y, \n",
    "                                         min_confs, max_confs, interval, n_reps, **kwargs)\n",
    "                elif method == 'kmeans':\n",
    "                    df_aucs = kmeans_picking_consensus(func, df_X, df_y, \n",
    "                                     min_confs, max_confs, interval, **kwargs)\n",
    "                else:\n",
    "                    print('Wrong method for conformations selection (\"random\", \"kmeans\").')\n",
    "                # Add to the dictionary of results\n",
    "                _results_dict[key_name] = df_aucs\n",
    "        # Saves the dictionary\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(_results_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return(_results_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
