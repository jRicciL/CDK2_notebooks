{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_results(path, column_to_drop, active_label='Active'):\n",
    "    '''Function to load VS results from path file.\n",
    "    It returns X and y ndarrays.'''\n",
    "    path_file = os.path.join(*path.split('/'))\n",
    "    df_results = pd.read_csv(path_file, index_col=0)\n",
    "    y_ = pd.Series(df_results[column_to_drop] == active_label, dtype = int) # Setting y_true\n",
    "    X_ = df_results.drop([column_to_drop], axis = 1) # Setting X\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, estimator, estimator_hyparams = None,\n",
    "               scoring = 'roc_auc', k_folds = 5, standarize = True, \n",
    "               split_train = False, test_size = 0.2, random_state = 1, **kwargs):\n",
    "    '''If desire, the original train set cab be splitted. Just in case of been useful'''\n",
    "    if split_train:\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train, y_train, test_size = test_size,\n",
    "                         stratify = y_train, random_state = random_state)\n",
    "    \n",
    "    '''\n",
    "     1) Pipeline is created, and will perform StandarScaler. More steps can be added later.\n",
    "     # pipe args is a list of tuples initialized with one element; the estimator.\n",
    "     # If standarize = true it adds StandardScaler at the bigining of the pipe.\n",
    "    '''\n",
    "    pipe_args = [(\"estimator\", estimator(**kwargs))]\n",
    "    if standarize:\n",
    "        pipe_args.insert(0, (\"scaler\", StandardScaler()))\n",
    "    pipe = Pipeline(pipe_args)\n",
    "    '''\n",
    "    2) The specific hyperparameters of the selected stimator are given, \n",
    "    we will parse them to the gridSearch instance.\n",
    "    '''\n",
    "    if estimator_hyparams != None:\n",
    "        params = {}\n",
    "        for key, value in estimator_hyparams.items():\n",
    "            params['estimator__' + key] = value\n",
    "        '''\n",
    "        3) Grid search cross validation for turning the optimal parameters, \n",
    "        it takes the pipeline object. GridSearch performs k-fold cross validation, \n",
    "        and uses the given scoring method to validate each set.\n",
    "\n",
    "        '''\n",
    "        grid = GridSearchCV(estimator = pipe, param_grid = params, \n",
    "                            cv = k_folds, scoring = scoring,\n",
    "                            n_jobs = 6, refit=True)\n",
    "        estimator = grid\n",
    "        \n",
    "    else:\n",
    "        '''Additionaly, if estimator_hyparams is None, Grid search is avoided.'''\n",
    "        estimator = pipe\n",
    "        \n",
    "    '''SVC training through GridSearch object'''\n",
    "    estimator.fit(X_train, y_train)\n",
    "    '''Return the trained estimator (an instance from GridSearchCV or Pipeline)'''\n",
    "    #final_model = estimator.best_estimator_ if estimator_hyparams != None else estimator\n",
    "    return(estimator)\n",
    "\n",
    "def eval_model(model, X_test, y_test, return_proba = True):\n",
    "    '''\n",
    "    1) Predictions and evaluation on the Test set\n",
    "    - Scaling and prediction of X_train using the best model found by grid\n",
    "    '''\n",
    "    if return_proba:\n",
    "        y_prob  = model.predict_proba(X_test)[:,1] # Predicted prob values for X_test\n",
    "        y_hat = y_prob\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "        y_hat = y_score\n",
    "    y_pred  = model.predict(X_test) # predicted values\n",
    "    '''Returns the y_score values and the lnear_SVC object'''\n",
    "    return(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(train_key, list_of_scores, estimator,\n",
    "                  scores_dic, selected_features = None, **kwargs):\n",
    "    ''''''\n",
    "    trained_models = {}\n",
    "    for score in list_of_scores:\n",
    "        if selected_features is None:\n",
    "            X_train = scores_dic[train_key][score]['X']\n",
    "        else:\n",
    "            X_train = scores_dic[train_key][score]['X'][selected_features]\n",
    "        y_train = scores_dic[train_key][score]['y']\n",
    "        name = F'{score}'\n",
    "        trained_models[name] = train_model(X_train, y_train, estimator, **kwargs)\n",
    "    return(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_wrapper(trained_model, test_keys, list_of_scores,\n",
    "                  scores_dic, selected_features = None, return_proba = True, **kwargs):\n",
    "    model_results = {}\n",
    "    for test_key in test_keys:\n",
    "        y_preds = {}\n",
    "        for score in list_of_scores:\n",
    "            if selected_features is None:\n",
    "                X_test  = scores_dic[test_key][score]['X']\n",
    "            else: \n",
    "                X_test  = scores_dic[test_key][score]['X'][selected_features]\n",
    "            y_test  = scores_dic[test_key][score]['y']\n",
    "            name = F'{test_key}-{score}'\n",
    "            y_preds[name] = eval_model(trained_model[score], X_test, y_test, return_proba = return_proba)\n",
    "        # Invoke PlotMetric Class\n",
    "        model_results[test_key] = PlotMetric(y_true = y_test, y_pred_dict = y_preds, **kwargs)\n",
    "    return(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_results(title, predictions, train_key, plot_rankings = True, plot_nef = False):\n",
    "    n_rows = 2 if plot_nef else 1\n",
    "    plt.figure(figsize=(14, 7*n_rows))\n",
    "    #plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for i, test_set in enumerate(test_keys):\n",
    "        plt.subplot(F'{n_rows}2{i+1}')\n",
    "        predictions[test_set].plot_roc_auc(F'{title}:\\n{train_key} train, {test_set} test', \n",
    "                                     show_by_itself = False, fontsize = 'x-small')\n",
    "        if plot_nef:\n",
    "            plt.subplot(F'{n_rows}2{i+3}')\n",
    "            predictions[test_set].plot_ef_auc('', method = 'normalized', max_chi = 0.1, \n",
    "                                         show_by_itself = False, fontsize = 'x-small')\n",
    "    plt.show()\n",
    "    if plot_rankings:\n",
    "        for test_set in test_keys:\n",
    "            predictions[test_set].plot_actives_distribution(max_position_to_plot=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for random picking n features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_confs_picking(scores_dic,\n",
    "                             score_type,\n",
    "                             train_key, \n",
    "                             test_keys,\n",
    "                             model, \n",
    "                             n_features,\n",
    "                             n_reps = 30,\n",
    "                             metric = 'roc_auc',\n",
    "                             **kwargs,\n",
    "                            ):\n",
    "    # Creates the dictionary of results to be filled and returned\n",
    "    results_dict = {}\n",
    "    for test in test_keys:\n",
    "        results_dict[test] = np.zeros(n_reps)\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        '''Performs the random selection'''\n",
    "        random_features = np.random.choice(a = range(0, 402), size = n_features, replace=False)\n",
    "        \n",
    "        random_features = np.sort(random_features)\n",
    "        features = scores_dic[train_key][score_type]['X'].columns[random_features]\n",
    "        \n",
    "        model_train = train_wrapper(train_key = train_key, list_of_scores = [score_type],\n",
    "                               scores_dic = scores_dic, estimator = model,\n",
    "                               selected_features = features,\n",
    "                               **kwargs)\n",
    "        \n",
    "        model_pred = eval_wrapper(trained_model = model_train, \n",
    "                            test_keys = test_keys, list_of_scores = [score_type], \n",
    "                            selected_features = features,\n",
    "                            scores_dic = scores_dic, decreasing = False)\n",
    "        \n",
    "        for test in test_keys:\n",
    "            metric_value = model_pred[test].format_metric_results(metric).values[0][0]\n",
    "            results_dict[test][rep] = metric_value\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtion to perform kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_medoids_idx(X, n_dims = 2, n_clusters = 10, random_state = None):\n",
    "    _X = X[:fd].T\n",
    "\n",
    "    kmc = KMeans(n_clusters = n_clusters, random_state = random_state)\n",
    "    kmc.fit( _X )\n",
    "    \n",
    "    # Get the distances from the centroids\n",
    "    _y = kmc.labels_\n",
    "    medoids_idx = []\n",
    "    for label in sorted(np.unique(_y)):\n",
    "        label_indices = np.where(_y == label)[0]\n",
    "        # Get the distance from the centroid\n",
    "        # centroid\n",
    "        centroid = kmc.cluster_centers_[label]\n",
    "        # Points inside the cluster\n",
    "        _X_points = _X[label_indices]\n",
    "        # Compute the distances\n",
    "        _X_distances = ((_X_points - centroid)**2).sum(axis = 1)\n",
    "        # Get the closest point\n",
    "        medoid_idx = label_indices[np.argmin(_X_distances)]\n",
    "        medoids_idx.append(medoid_idx)\n",
    "\n",
    "    return medoids_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
