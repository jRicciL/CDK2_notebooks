{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
    "\n",
    "def _validate(clf, clf_name, \n",
    "                X_test, y_test,\n",
    "                metric_name, metric_params):\n",
    "    if clf_name.startswith('ml_'):\n",
    "        # Make the predictions\n",
    "        y_pred = clf.predict_proba(X_test)[:,1]\n",
    "    elif clf_name.startswith('cs_'):\n",
    "        # apply consensus\n",
    "        y_pred = clf(pd.DataFrame(X_test))\n",
    "    else:\n",
    "        print(clf_name, 'not found. Ommited.')\n",
    "        return\n",
    "    \n",
    "    # Make the evaluation\n",
    "    metric = PlotMetric(y_test, {'': y_pred},\n",
    "                decreasing=False)\\\n",
    "                .format_metric_results(\n",
    "                    rounded=5,\n",
    "                    #metric_name=metric_name,\n",
    "        # metric_params already includes metric_name\n",
    "                    **metric_params)\n",
    "\n",
    "    return metric.values[0][0]\n",
    "\n",
    "\n",
    "def _train_cfl(clf, X_train, y_train):\n",
    "    # Fit the estimator\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "def _do_replicates(splits, \n",
    "                   estimators, X, y,\n",
    "                   metrics):\n",
    "    results={}\n",
    "    # Machine Learning Classifiers\n",
    "    for clf_name, clf in estimators.items():\n",
    "        folds = []\n",
    "        for i, (train, test) in enumerate(splits):\n",
    "            if clf_name.startswith('ml_'):\n",
    "                # Fit the ml classifier once per fold\n",
    "                cfl = _train_cfl(clf, X[train], y[train])\n",
    "            \n",
    "            # for each metric\n",
    "#             metric_results = {}\n",
    "            for metric_name, metric_params in metrics.items():\n",
    "            \n",
    "                metric = _validate(\n",
    "                    clf, clf_name, \n",
    "                    X[test], y[test],\n",
    "                    metric_name, metric_params\n",
    "                )\n",
    "                # Append the results\n",
    "                folds.append(metric)\n",
    "\n",
    "        # Add to the results dictonary \n",
    "        results[clf_name] = folds\n",
    "\n",
    "    return results\n",
    "\n",
    "def _format_results_to_df(metrics, results, n):\n",
    "        # Format into a dataframe\n",
    "    # Create the metric names and repeat them \n",
    "    n_metrics = len(metrics)\n",
    "    index_names = [*metrics.keys()]*n\n",
    "    \n",
    "    # convert to a dataframe\n",
    "    df_res = pd.DataFrame(\n",
    "        results, \n",
    "        index= pd.MultiIndex.from_tuples(\n",
    "            zip(index_names,\n",
    "                np.repeat(range(n), n_metrics))\n",
    "        ))\n",
    "    df_res = df_res.sort_index()\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "\n",
    "@cached()\n",
    "def k_cross_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_splits=5, \n",
    "          random_state=None, \n",
    "          shuffle=True):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = StratifiedKFold(n_splits=n_splits, \n",
    "                         random_state=random_state,\n",
    "                         shuffle=shuffle)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results = _do_replicates(splits, estimators, X, y, \n",
    "                             metrics)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, n=n_splits)\n",
    "    \n",
    "    return df_res \n",
    "\n",
    "\n",
    "@cached()\n",
    "def n_hold_out_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_reps=5, test_size=0.25,\n",
    "          random_state=None):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = StratifiedShuffleSplit(\n",
    "                        n_splits=n_reps, \n",
    "                        test_size=test_size,\n",
    "                        random_state=random_state)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results = _do_replicates(splits, estimators, X, y,\n",
    "          metrics)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, n=n_reps)\n",
    "    \n",
    "    return df_res \n",
    "\n",
    "\n",
    "@cached()\n",
    "def nk_rep_cross_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_splits=2, \n",
    "          n_repeats=5,\n",
    "          random_state=None, \n",
    "          shuffle=True):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "                         n_splits=n_splits,\n",
    "                         n_repeats=n_repeats,\n",
    "                         random_state=random_state)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results = _do_replicates(splits, estimators, X, y, \n",
    "                             metrics)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, n=n_splits*n_repeats)\n",
    "    \n",
    "    return df_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "@cached()\n",
    "def plot_roc_cv(classifier, X, y, random_state=None,\n",
    "               n_folds=5, ax=None, name=''):\n",
    "    sns.set(style='whitegrid', font_scale=1.2)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X[train], y[train])\n",
    "        viz = plot_roc_curve(classifier, X[test], y[test],\n",
    "                             name='ROC fold {}'.format(i),\n",
    "                             alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Random', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "    ax.set_title(label=f\"{n_folds}-fold CV ROC curve: {name}\", fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, bartlett, levene\n",
    "\n",
    "def norm_test(x, alpha=0.05):\n",
    "    s, p = shapiro(x)\n",
    "    result = 'rejected' if p < alpha else 'accepted'\n",
    "    print(f'The H0 is {result} => (W={round(s, 3)}, p={round(p, 3)})')\n",
    "    \n",
    "def homovar_test(x, y, alpha=0.05):\n",
    "    s, p = bartlett(x, y)\n",
    "    result = 'rejected' if p < alpha else 'accepted'\n",
    "    print(f'The H0 is {result} => (W={round(s, 3)}, p={round(p, 3)})')\n",
    "    \n",
    "def multi_norm_test(df, metric='roc_auc', alpha=0.05):\n",
    "    res = df.loc[metric].apply(shapiro, axis=0)\n",
    "    return pd.DataFrame([0 if i[1] < alpha else 1 for i in res], \n",
    "             index=df.columns, columns=['Normality']).T\n",
    "\n",
    "def multi_homovar_test(df, metric='roc_auc', alpha=0.05, as_df=True):\n",
    "    res = bartlett(*df.loc[metric].values)\n",
    "    if as_df:\n",
    "        res = pd.DataFrame(res, columns=['Bartlett'], \n",
    "                           index=['statistic', 'p']).T\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory memoization\n",
    "from joblib import Memory\n",
    "location = './cachedir'\n",
    "memory = Memory(location, verbose=0)\n",
    "\n",
    "n_hold_out_validation = memory.cache(n_hold_out_validation) \n",
    "\n",
    "k_cross_validation = memory.cache(k_cross_validation) \n",
    "\n",
    "nk_rep_cross_validation = memory.cache(nk_rep_cross_validation) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
